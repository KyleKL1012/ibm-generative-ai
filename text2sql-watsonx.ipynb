{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZZ0CTQ672\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import transformers\n",
    "import einops\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "hf_hub = os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
    "hf_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'google/flan-t5-large'\n",
    "model_kwargs = {\"temperature\": 0.5, \"max_length\": 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceHub(repo_id = model,  model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/google/flan-t5-large', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='google/flan-t5-large', task=None, model_kwargs={'temperature': 0.5, 'max_length': 300}, huggingfacehub_api_token=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  SELECT T1.Creation_Year , T1.Name , T1.Budget FROM department AS T1 JOIN department_ranking AS T2 ON T1.Department_ID = T2.Department_ID\n"
     ]
    }
   ],
   "source": [
    "chat_history =[]\n",
    "topic = 'Text2SQL'\n",
    "\n",
    "template = \"\"\"\n",
    "        You are a SQL export to generate SQL query based on Table Schema below.\n",
    "\n",
    "        Table \"department\" Schema: CREATE TABLE IF NOT EXISTS \"department\" (\n",
    "                            \"Department_ID\" int,\n",
    "                            \"Name\" text,\n",
    "                            \"Creation\" text,\n",
    "                            \"Ranking\" int,\n",
    "                            \"Budget_in_Billions\" real,\n",
    "                            \"Num_Employees\" real,\n",
    "                            PRIMARY KEY (\"Department_ID\")\n",
    "                        );\n",
    "\n",
    "        Here are some examples:\n",
    "        Question: How many heads of the departments are older than 56 ?\n",
    "        Query: SELECT count(*) FROM head WHERE age  >  56\n",
    "\n",
    "        Question: List the name, born state and age of the heads of departments ordered by age.\n",
    "        Query: SELECT name ,  born_state ,  age FROM head ORDER BY age\n",
    "\n",
    "        Question:{question}\n",
    "        Query:\n",
    "    \"\"\"\n",
    "question = \"List the creation year, name and budget of each department.\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=model)\n",
    "result = llm_chain.run(question)\n",
    "\n",
    "\n",
    "print(\"Chatbot: \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "task_prefix = \"translate English to German: \"\n",
    "# use different length sentences to test batching\n",
    "sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n",
    "\n",
    "inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to  watsonx bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have a .env file under bampy root with\n",
    "# BAM_KEY=<your-bam-key>\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"BAM_KEY\")\n",
    "api_key = 'pak-ZBytb4IC5c5Fa-_ijvlglLeUTQ2LlnkZr-zuAaVhhGU'\n",
    "creds = Credentials(api_key) # credentials object to access BAM\n",
    "\n",
    "# Instantiate parameters for text generation\n",
    "params = GenerateParams(decoding_method=\"sample\", max_new_tokens=100)\n",
    "\n",
    "# Instantiate a model proxy object to send your requests\n",
    "flan_ul2 = Model(ModelType.FLAN_UL2, params=params, credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pak-ZBytb4IC5c5Fa-_ijvlglLeUTQ2LlnkZr-zuAaVhhGU\n"
     ]
    }
   ],
   "source": [
    "print(api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esay prompt test(还没完全调好，只是能跑罢了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SUFFIX = \"\"\"Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\", \"top_k\"],\n",
    "    template=_DEFAULT_TEMPLATE + PROMPT_SUFFIX,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_DECIDER_TEMPLATE = \"\"\"Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Table Names: {table_names}\n",
    "\n",
    "Relevant Table Names:\"\"\"\n",
    "DECIDER_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"table_names\"],\n",
    "    template=_DECIDER_TEMPLATE,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 10 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables:\\nCUSTOMER_MASTER\\n\\nQuestion: How many customers are there in the CUSTOMER_MASTER table?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sqlite_prompt = \"\"\"You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "SQLITE_PROMPT = PromptTemplate(\n",
    "    input_variables = [\"input\", \"table_info\", \"top_k\"],\n",
    "    template = _sqlite_prompt + PROMPT_SUFFIX\n",
    ")\n",
    "sqlite_prompt = SQLITE_PROMPT.format(input = \"How many customers are there in the CUSTOMER_MASTER table?\", table_info = \"CUSTOMER_MASTER\", top_k = 10)\n",
    "sqlite_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(*) FROM CUSTOMER_MASTER\n"
     ]
    }
   ],
   "source": [
    "prompts = [sqlite_prompt]\n",
    "for response in flan_ul2.generate(prompts):\n",
    "    print(response.generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
